{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keybert-all-mpnet-base-v2 모델 사용해 Keybert 추출함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kadv1_rob1=pd.read_csv('/home/dxlab/jupyter/euiseog/metaverse/Network-Analysis-Metaverse/datasets/keybert-all-mpnet-base-v2/roblox1.csv',index_col=0)\n",
    "kadv1_rob2=pd.read_csv('/home/dxlab/jupyter/euiseog/metaverse/Network-Analysis-Metaverse/datasets/keybert-all-mpnet-base-v2/roblox2.csv',index_col=0)\n",
    "kadv1_rob3=pd.read_csv('/home/dxlab/jupyter/euiseog/metaverse/Network-Analysis-Metaverse/datasets/keybert-all-mpnet-base-v2/roblox3.csv',index_col=0)\n",
    "kadv1_rob4=pd.read_csv('/home/dxlab/jupyter/euiseog/metaverse/Network-Analysis-Metaverse/datasets/keybert-all-mpnet-base-v2/roblox4.csv',index_col=0)\n",
    "kadv1_rob5=pd.read_csv('/home/dxlab/jupyter/euiseog/metaverse/Network-Analysis-Metaverse/datasets/keybert-all-mpnet-base-v2/roblox5.csv',index_col=0)\n",
    "kadv1_zep=pd.read_csv('/home/dxlab/jupyter/euiseog/metaverse/Network-Analysis-Metaverse/datasets/keybert-all-mpnet-base-v2/zepeto.csv',index_col=0)\n",
    "\n",
    "# 스트링을 리스트로 변환하는 함수\n",
    "def convert_string_to_list(s):\n",
    "    try:\n",
    "        return literal_eval(s)\n",
    "    except (ValueError, SyntaxError):  # 변환할 수 없는 경우 스트링 그대로 반환\n",
    "        return s\n",
    "    \n",
    "def keywords_extracter(x):\n",
    "    only_keywords=[]\n",
    "    for each_tup in x:\n",
    "        only_keywords.append(each_tup[0])\n",
    "    return only_keywords\n",
    "    \n",
    "kadv1_rob1['keybert_keywords']=kadv1_rob1['keybert_keywords'].apply(convert_string_to_list)\n",
    "kadv1_rob2['keybert_keywords']=kadv1_rob2['keybert_keywords'].apply(convert_string_to_list)\n",
    "kadv1_rob3['keybert_keywords']=kadv1_rob3['keybert_keywords'].apply(convert_string_to_list)\n",
    "kadv1_rob4['keybert_keywords']=kadv1_rob4['keybert_keywords'].apply(convert_string_to_list)\n",
    "kadv1_rob5['keybert_keywords']=kadv1_rob5['keybert_keywords'].apply(convert_string_to_list)\n",
    "kadv1_zep['keybert_keywords']=kadv1_zep['keybert_keywords'].apply(convert_string_to_list)\n",
    "\n",
    "kadv1_rob1['keywords']=kadv1_rob1['keybert_keywords'].apply(keywords_extracter)\n",
    "kadv1_rob2['keywords']=kadv1_rob2['keybert_keywords'].apply(keywords_extracter)\n",
    "kadv1_rob3['keywords']=kadv1_rob3['keybert_keywords'].apply(keywords_extracter)\n",
    "kadv1_rob4['keywords']=kadv1_rob4['keybert_keywords'].apply(keywords_extracter)\n",
    "kadv1_rob5['keywords']=kadv1_rob5['keybert_keywords'].apply(keywords_extracter)\n",
    "kadv1_zep['keywords']=kadv1_zep['keybert_keywords'].apply(keywords_extracter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1_keybert=kadv1_rob1['keybert_keywords'].values\n",
    "r2_keybert=kadv1_rob2['keybert_keywords'].values\n",
    "r3_keybert=kadv1_rob3['keybert_keywords'].values\n",
    "r4_keybert=kadv1_rob4['keybert_keywords'].values\n",
    "r5_keybert=kadv1_rob5['keybert_keywords'].values\n",
    "z_keybert=kadv1_zep['keybert_keywords'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1_keywords=np.concatenate(kadv1_rob1['keywords'])\n",
    "r2_keywords=np.concatenate(kadv1_rob2['keywords'])\n",
    "r3_keywords=np.concatenate(kadv1_rob3['keywords'])\n",
    "r4_keywords=np.concatenate(kadv1_rob4['keywords'])\n",
    "r5_keywords=np.concatenate(kadv1_rob5['keywords'])\n",
    "z_keywords=np.concatenate(kadv1_zep['keywords'])\n",
    "\n",
    "unique_r1_keywords=np.unique(r1_keywords)\n",
    "unique_r2_keywords=np.unique(r2_keywords)\n",
    "unique_r3_keywords=np.unique(r3_keywords)\n",
    "unique_r4_keywords=np.unique(r4_keywords)\n",
    "unique_r5_keywords=np.unique(r5_keywords)\n",
    "\n",
    "unique_r_keywords=np.unique(np.concatenate([unique_r1_keywords,unique_r2_keywords,unique_r3_keywords,unique_r4_keywords,unique_r5_keywords]))\n",
    "unique_z_keywords=np.unique(z_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 알파벳만 남기기\n",
    "\n",
    "r1_keybert_v0=[[tup for tup in r1_keybert[i] if tup[0].isalpha()] for i in range(len(r1_keybert))]\n",
    "r2_keybert_v0=[[tup for tup in r2_keybert[i] if tup[0].isalpha()] for i in range(len(r2_keybert))] \n",
    "r3_keybert_v0=[[tup for tup in r3_keybert[i] if tup[0].isalpha()] for i in range(len(r3_keybert))]\n",
    "r4_keybert_v0=[[tup for tup in r4_keybert[i] if tup[0].isalpha()] for i in range(len(r4_keybert))] \n",
    "r5_keybert_v0=[[tup for tup in r5_keybert[i] if tup[0].isalpha()] for i in range(len(r5_keybert))]\n",
    "z_keybert_v0=[[tup for tup in z_keybert[i] if tup[0].isalpha()] for i in range(len(z_keybert))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 글자 수 4~14 단어들만 남기기 \n",
    "\n",
    "r1_keybert_v1=[[tup for tup in r1_keybert[i] if 3<len(tup[0])<15] for i in range(len(r1_keybert_v0))]\n",
    "r2_keybert_v1=[[tup for tup in r2_keybert[i] if 3<len(tup[0])<15] for i in range(len(r2_keybert_v0))] \n",
    "r3_keybert_v1=[[tup for tup in r3_keybert[i] if 3<len(tup[0])<15] for i in range(len(r3_keybert_v0))]\n",
    "r4_keybert_v1=[[tup for tup in r4_keybert[i] if 3<len(tup[0])<15] for i in range(len(r4_keybert_v0))] \n",
    "r5_keybert_v1=[[tup for tup in r5_keybert[i] if 3<len(tup[0])<15] for i in range(len(r5_keybert_v0))]\n",
    "z_keybert_v1=[[tup for tup in z_keybert[i] if 3<len(tup[0])<15] for i in range(len(z_keybert_v0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 단어(토큰) 수 5 미만 제거\n",
    "\n",
    "r1_keybert_v1=[tuple_list for tuple_list in r1_keybert_v1 if len(tuple_list)>4]\n",
    "r2_keybert_v1=[tuple_list for tuple_list in r2_keybert_v1 if len(tuple_list)>4]\n",
    "r3_keybert_v1=[tuple_list for tuple_list in r3_keybert_v1 if len(tuple_list)>4]\n",
    "r4_keybert_v1=[tuple_list for tuple_list in r4_keybert_v1 if len(tuple_list)>4]\n",
    "r5_keybert_v1=[tuple_list for tuple_list in r5_keybert_v1 if len(tuple_list)>4]\n",
    "z_keybert_v1=[tuple_list for tuple_list in z_keybert_v1 if len(tuple_list)>4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dxlab/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/dxlab/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## lemmatization\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "r1_keybert_v2=[[(lemmatizer.lemmatize(tup[0]),tup[1]) for tup in r1_keybert_v1[i]] for i in range(len(r1_keybert_v1))]\n",
    "r2_keybert_v2=[[(lemmatizer.lemmatize(tup[0]),tup[1]) for tup in r2_keybert_v1[i]] for i in range(len(r2_keybert_v1))]\n",
    "r3_keybert_v2=[[(lemmatizer.lemmatize(tup[0]),tup[1]) for tup in r3_keybert_v1[i]] for i in range(len(r3_keybert_v1))]\n",
    "r4_keybert_v2=[[(lemmatizer.lemmatize(tup[0]),tup[1]) for tup in r4_keybert_v1[i]] for i in range(len(r4_keybert_v1))]\n",
    "r5_keybert_v2=[[(lemmatizer.lemmatize(tup[0]),tup[1]) for tup in r5_keybert_v1[i]] for i in range(len(r5_keybert_v1))]\n",
    "z_keybert_v2=[[(lemmatizer.lemmatize(tup[0]),tup[1]) for tup in z_keybert_v1[i]] for i in range(len(z_keybert_v1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93316/93316 [00:00<00:00, 277552.12it/s]\n",
      "100%|██████████| 90629/90629 [00:00<00:00, 290236.89it/s]\n",
      "100%|██████████| 91741/91741 [00:02<00:00, 44542.51it/s] \n",
      "100%|██████████| 75557/75557 [00:00<00:00, 327029.95it/s]\n",
      "100%|██████████| 30776/30776 [00:00<00:00, 326892.34it/s]\n",
      "100%|██████████| 39420/39420 [00:00<00:00, 305191.75it/s]\n"
     ]
    }
   ],
   "source": [
    "## 중복 단어 제거 \n",
    "def v2_5_extracter(v2):\n",
    "    \n",
    "    v2_5=[]\n",
    "    for tuple_list in tqdm(v2):\n",
    "        old_dict=dict(tuple_list)\n",
    "        new_dict=dict()\n",
    "\n",
    "        for item,val in old_dict.items():\n",
    "            try:\n",
    "                if new_dict[item]<val:\n",
    "                    new_dict[item]=val\n",
    "            except:\n",
    "                new_dict[item]=val\n",
    "        \n",
    "        v2_5.append(sorted(list(new_dict.items())))\n",
    "    return v2_5\n",
    "\n",
    "r1_keybert_v2_5=v2_5_extracter(r1_keybert_v2)\n",
    "r2_keybert_v2_5=v2_5_extracter(r2_keybert_v2)\n",
    "r3_keybert_v2_5=v2_5_extracter(r3_keybert_v2)\n",
    "r4_keybert_v2_5=v2_5_extracter(r4_keybert_v2)\n",
    "r5_keybert_v2_5=v2_5_extracter(r5_keybert_v2)\n",
    "z_keybert_v2_5=v2_5_extracter(z_keybert_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dxlab/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/dxlab/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "100%|██████████| 93316/93316 [00:27<00:00, 3452.64it/s]\n",
      "100%|██████████| 90629/90629 [00:26<00:00, 3412.37it/s]\n",
      "100%|██████████| 91741/91741 [00:25<00:00, 3550.13it/s]\n",
      "100%|██████████| 75557/75557 [00:19<00:00, 3881.16it/s]\n",
      "100%|██████████| 30776/30776 [00:07<00:00, 3988.42it/s]\n",
      "100%|██████████| 39420/39420 [00:11<00:00, 3572.97it/s]\n"
     ]
    }
   ],
   "source": [
    "## NN~ extraction\n",
    "\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def v3_extracter(v2_5):\n",
    "    v3=[]\n",
    "    for tuples in tqdm(v2_5):\n",
    "        only_words=[tup[0] for tup in tuples]\n",
    "        tagged_words=pos_tag(only_words)\n",
    "        v3.append( [tup for i,tup in enumerate(tuples) if tagged_words[i][1].startswith('NN')] )\n",
    "    return v3\n",
    "\n",
    "r1_keybert_v3=v3_extracter(r1_keybert_v2_5)\n",
    "r2_keybert_v3=v3_extracter(r2_keybert_v2_5)\n",
    "r3_keybert_v3=v3_extracter(r3_keybert_v2_5)\n",
    "r4_keybert_v3=v3_extracter(r4_keybert_v2_5)\n",
    "r5_keybert_v3=v3_extracter(r5_keybert_v2_5)\n",
    "z_keybert_v3=v3_extracter(z_keybert_v2_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 엣지 구성 못하는 1개 미만인 리뷰 제거\n",
    "\n",
    "r1_keybert_v4=[lists for lists in r1_keybert_v3 if len(lists)>1]\n",
    "r2_keybert_v4=[lists for lists in r2_keybert_v3 if len(lists)>1]\n",
    "r3_keybert_v4=[lists for lists in r3_keybert_v3 if len(lists)>1]\n",
    "r4_keybert_v4=[lists for lists in r4_keybert_v3 if len(lists)>1]\n",
    "r5_keybert_v4=[lists for lists in r5_keybert_v3 if len(lists)>1]\n",
    "z_keybert_v4=[lists for lists in z_keybert_v3 if len(lists)>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92326/92326 [00:00<00:00, 623759.00it/s]\n",
      "100%|██████████| 89680/89680 [00:00<00:00, 653489.32it/s]\n",
      "100%|██████████| 89734/89734 [00:00<00:00, 679040.31it/s]\n",
      "100%|██████████| 72566/72566 [00:00<00:00, 775856.54it/s]\n",
      "100%|██████████| 29431/29431 [00:00<00:00, 758586.84it/s]\n",
      "100%|██████████| 38062/38062 [00:00<00:00, 571832.40it/s]\n"
     ]
    }
   ],
   "source": [
    "## keybert 값 기준 상위 단어 200개 추출\n",
    "\n",
    "r_dict=dict()\n",
    "z_dict=dict()\n",
    "\n",
    "def dict_extracter(dic,v4):\n",
    "    for tuple_list in tqdm(v4):\n",
    "        for word, score in tuple_list:\n",
    "            try:\n",
    "                if dic[word]<score:\n",
    "                    dic[word]=score\n",
    "                else:\n",
    "                    pass\n",
    "            except:\n",
    "                dic[word]=score\n",
    "    return dic\n",
    "\n",
    "r_dict=dict_extracter(r_dict,r1_keybert_v4)\n",
    "r_dict=dict_extracter(r_dict,r2_keybert_v4)\n",
    "r_dict=dict_extracter(r_dict,r3_keybert_v4)\n",
    "r_dict=dict_extracter(r_dict,r4_keybert_v4)\n",
    "r_dict=dict_extracter(r_dict,r5_keybert_v4)\n",
    "z_dict=dict_extracter(z_dict,z_keybert_v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "top_words_r = sorted(r_dict.items(), key=itemgetter(1), reverse=True)[:200]\n",
    "top_words_z = sorted(z_dict.items(), key=itemgetter(1), reverse=True)[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top_words_z: isalpha()로 걸러지지 않은 단어들이 keybert 상위값을 차지하고 있어서 정상적인 노드 추출 불가"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "es_py38",
   "language": "python",
   "name": "es_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
