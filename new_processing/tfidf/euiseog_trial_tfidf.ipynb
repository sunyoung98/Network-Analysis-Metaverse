{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/dxlab/jupyter/euiseog/metaverse'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2370/1619515545.py:2: DtypeWarning: Columns (9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  roblox2=pd.read_csv('/home/dxlab/jupyter/euiseog/metaverse/Network-Analysis-Metaverse/datasets/original/roblox2.csv',index_col=0)\n",
      "/tmp/ipykernel_2370/1619515545.py:3: DtypeWarning: Columns (9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  roblox3=pd.read_csv('/home/dxlab/jupyter/euiseog/metaverse/Network-Analysis-Metaverse/datasets/original/roblox3.csv',index_col=0)\n",
      "/tmp/ipykernel_2370/1619515545.py:4: DtypeWarning: Columns (9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  roblox4=pd.read_csv('/home/dxlab/jupyter/euiseog/metaverse/Network-Analysis-Metaverse/datasets/original/roblox4.csv',index_col=0)\n"
     ]
    }
   ],
   "source": [
    "roblox1=pd.read_csv('/home/dxlab/jupyter/euiseog/metaverse/Network-Analysis-Metaverse/datasets/original/roblox1.csv',index_col=0)\n",
    "roblox2=pd.read_csv('/home/dxlab/jupyter/euiseog/metaverse/Network-Analysis-Metaverse/datasets/original/roblox2.csv',index_col=0)\n",
    "roblox3=pd.read_csv('/home/dxlab/jupyter/euiseog/metaverse/Network-Analysis-Metaverse/datasets/original/roblox3.csv',index_col=0)\n",
    "roblox4=pd.read_csv('/home/dxlab/jupyter/euiseog/metaverse/Network-Analysis-Metaverse/datasets/original/roblox4.csv',index_col=0)\n",
    "roblox5=pd.read_csv('/home/dxlab/jupyter/euiseog/metaverse/Network-Analysis-Metaverse/datasets/original/roblox5.csv',index_col=0)\n",
    "zepeto=pd.read_csv('/home/dxlab/jupyter/euiseog/metaverse/Network-Analysis-Metaverse/datasets/original/zepeto.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dxlab/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "100%|██████████| 115407/115407 [00:10<00:00, 11367.69it/s]\n"
     ]
    }
   ],
   "source": [
    "### tokenization\n",
    "\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "nltk.download('punkt')  # 필요한 데이터를 다운로드합니다.\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "r1_content=roblox1['content'].values\n",
    "r2_content=roblox2['content'].values\n",
    "r3_content=roblox3['content'].values\n",
    "r4_content=roblox4['content'].values\n",
    "r5_content=roblox5['content'].values\n",
    "z_content=zepeto['content'].values\n",
    "\n",
    "r1_words_v0=[word_tokenize(r1_content[i]) for i in range(len(r1_content))]\n",
    "r2_words_v0=[word_tokenize(r2_content[i]) for i in range(len(r2_content))]\n",
    "r3_words_v0=[word_tokenize(r3_content[i]) for i in range(len(r3_content))]\n",
    "r4_words_v0=[word_tokenize(r4_content[i]) for i in range(len(r4_content))]\n",
    "r5_words_v0=[word_tokenize(r5_content[i]) for i in range(len(r5_content))]\n",
    "\n",
    "z_words_v0=[]  # nan값 때문에 따로 처리\n",
    "        \n",
    "for content in tqdm(z_content):\n",
    "    try:\n",
    "        z_words_v0.append(word_tokenize(content))\n",
    "    except:  \n",
    "        z_words_v0.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 알파벳만 남기기\n",
    "\n",
    "r1_words_v1=[[tup for tup in r1_words_v0[i] if tup.isalpha()] for i in range(len(r1_words_v0))]\n",
    "r2_words_v1=[[tup for tup in r2_words_v0[i] if tup.isalpha()] for i in range(len(r2_words_v0))] \n",
    "r3_words_v1=[[tup for tup in r3_words_v0[i] if tup.isalpha()] for i in range(len(r3_words_v0))]\n",
    "r4_words_v1=[[tup for tup in r4_words_v0[i] if tup.isalpha()] for i in range(len(r4_words_v0))] \n",
    "r5_words_v1=[[tup for tup in r5_words_v0[i] if tup.isalpha()] for i in range(len(r5_words_v0))]\n",
    "z_words_v1=[[tup for tup in z_words_v0[i] if tup.isalpha()] for i in range(len(z_words_v0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 글자 수 4~14 단어들만 남기기 \n",
    "\n",
    "r1_words_v2=[[tup for tup in r1_words_v1[i] if 3<len(tup)<15] for i in range(len(r1_words_v1))]\n",
    "r2_words_v2=[[tup for tup in r2_words_v1[i] if 3<len(tup)<15] for i in range(len(r2_words_v1))] \n",
    "r3_words_v2=[[tup for tup in r3_words_v1[i] if 3<len(tup)<15] for i in range(len(r3_words_v1))]\n",
    "r4_words_v2=[[tup for tup in r4_words_v1[i] if 3<len(tup)<15] for i in range(len(r4_words_v1))] \n",
    "r5_words_v2=[[tup for tup in r5_words_v1[i] if 3<len(tup)<15] for i in range(len(r5_words_v1))]\n",
    "z_words_v2=[[tup for tup in z_words_v1[i] if 3<len(tup)<15] for i in range(len(z_words_v1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 단어(토큰) 수 5 미만 제거\n",
    "\n",
    "r1_words_v2=[token_list for token_list in r1_words_v1 if len(token_list)>4]\n",
    "r2_words_v2=[token_list for token_list in r2_words_v1 if len(token_list)>4]\n",
    "r3_words_v2=[token_list for token_list in r3_words_v1 if len(token_list)>4]\n",
    "r4_words_v2=[token_list for token_list in r4_words_v1 if len(token_list)>4]\n",
    "r5_words_v2=[token_list for token_list in r5_words_v1 if len(token_list)>4]\n",
    "z_words_v2=[token_list for token_list in z_words_v1 if len(token_list)>4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dxlab/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/dxlab/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## lemmatization\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "r1_words_v3=[[lemmatizer.lemmatize(tup) for tup in r1_words_v2[i]] for i in range(len(r1_words_v2))]\n",
    "r2_words_v3=[[lemmatizer.lemmatize(tup) for tup in r2_words_v2[i]] for i in range(len(r2_words_v2))]\n",
    "r3_words_v3=[[lemmatizer.lemmatize(tup) for tup in r3_words_v2[i]] for i in range(len(r3_words_v2))]\n",
    "r4_words_v3=[[lemmatizer.lemmatize(tup) for tup in r4_words_v2[i]] for i in range(len(r4_words_v2))]\n",
    "r5_words_v3=[[lemmatizer.lemmatize(tup) for tup in r5_words_v2[i]] for i in range(len(r5_words_v2))]\n",
    "z_words_v3=[[lemmatizer.lemmatize(tup) for tup in z_words_v2[i]] for i in range(len(z_words_v2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 중복 단어 제거\n",
    "r1_words_v4=[list(set(instance)) for instance in r1_words_v3]\n",
    "r2_words_v4=[list(set(instance)) for instance in r2_words_v3]\n",
    "r3_words_v4=[list(set(instance)) for instance in r3_words_v3]\n",
    "r4_words_v4=[list(set(instance)) for instance in r4_words_v3]\n",
    "r5_words_v4=[list(set(instance)) for instance in r5_words_v3]\n",
    "z_words_v4=[list(set(instance)) for instance in z_words_v3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dxlab/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/dxlab/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "100%|██████████| 99906/99906 [01:29<00:00, 1122.36it/s]\n",
      "100%|██████████| 98891/98891 [01:12<00:00, 1361.75it/s]\n",
      "100%|██████████| 98771/98771 [00:57<00:00, 1717.99it/s]\n",
      "100%|██████████| 98800/98800 [00:43<00:00, 2249.36it/s]\n",
      "100%|██████████| 45870/45870 [00:18<00:00, 2459.16it/s]\n",
      "100%|██████████| 65128/65128 [00:32<00:00, 2026.07it/s]\n"
     ]
    }
   ],
   "source": [
    "## NN~ extraction\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def v5_extracter(v4):\n",
    "    v5=[]\n",
    "    for words in tqdm(v4):\n",
    "        tagged_words=pos_tag(words)\n",
    "        v5.append([word for word,tag in tagged_words if tag.startswith('NN')])\n",
    "    return v5\n",
    "\n",
    "\n",
    "r1_words_v5=v5_extracter(r1_words_v4)\n",
    "r2_words_v5=v5_extracter(r2_words_v4)\n",
    "r3_words_v5=v5_extracter(r3_words_v4)\n",
    "r4_words_v5=v5_extracter(r4_words_v4)\n",
    "r5_words_v5=v5_extracter(r5_words_v4)\n",
    "z_words_v5=v5_extracter(z_words_v4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r1_words_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './tfidf_data/top_words_r.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 58\u001b[0m\n\u001b[1;32m     54\u001b[0m top_words_z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(tfidf_dict_z, key\u001b[38;5;241m=\u001b[39mtfidf_dict_z\u001b[38;5;241m.\u001b[39mget, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[:\u001b[38;5;241m200\u001b[39m]\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./tfidf_data/top_words_r.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     59\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(top_words_r, file)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./tfidf_data/top_words_z.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "File \u001b[0;32m~/anaconda3/envs/es_py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './tfidf_data/top_words_r.pkl'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents_r=[]\n",
    "documents_z=[]\n",
    "\n",
    "for each_list in r1_words_v5:\n",
    "    documents_r.append(\" \".join(each_list))\n",
    "for each_list in r2_words_v5:\n",
    "    documents_r.append(\" \".join(each_list))\n",
    "for each_list in r3_words_v5:\n",
    "    documents_r.append(\" \".join(each_list))\n",
    "for each_list in r4_words_v5:\n",
    "    documents_r.append(\" \".join(each_list))\n",
    "for each_list in r5_words_v5:\n",
    "    documents_r.append(\" \".join(each_list))\n",
    "for each_list in z_words_v5:\n",
    "    documents_z.append(\" \".join(each_list))\n",
    "\n",
    "\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "vectorizer_r = TfidfVectorizer()\n",
    "vectorizer_z = TfidfVectorizer()\n",
    "tfidf_matrix_r = vectorizer_r.fit_transform(documents_r)\n",
    "tfidf_matrix_z = vectorizer_z.fit_transform(documents_z)\n",
    "\n",
    "# 단어 목록\n",
    "words_r = vectorizer_r.get_feature_names_out()\n",
    "words_z = vectorizer_z.get_feature_names_out()\n",
    "\n",
    "# TF-IDF 결과를 딕셔너리로 변환\n",
    "tfidf_dict_r = {}\n",
    "for doc_idx, doc in enumerate(documents_r):\n",
    "    feature_idx_r = tfidf_matrix_r[doc_idx].nonzero()[1]\n",
    "    tfidf_scores_r = zip(feature_idx_r, [tfidf_matrix_r[doc_idx, x] for x in feature_idx_r])\n",
    "    for word_idx, score in tfidf_scores_r:\n",
    "        word = words_r[word_idx]\n",
    "        tfidf_dict_r[word] = score\n",
    "\n",
    "# 상위 200개 단어 추출\n",
    "top_words_r = sorted(tfidf_dict_r, key=tfidf_dict_r.get, reverse=True)[:200]\n",
    "\n",
    "\n",
    "# TF-IDF 결과를 딕셔너리로 변환\n",
    "tfidf_dict_z = {}\n",
    "for doc_idx, doc in enumerate(documents_z):\n",
    "    feature_idx_z = tfidf_matrix_z[doc_idx].nonzero()[1]\n",
    "    tfidf_scores_z = zip(feature_idx_z, [tfidf_matrix_z[doc_idx, x] for x in feature_idx_z])\n",
    "    for word_idx, score in tfidf_scores_z:\n",
    "        word = words_z[word_idx]\n",
    "        tfidf_dict_z[word] = score\n",
    "\n",
    "# 상위 200개 단어 추출\n",
    "top_words_z = sorted(tfidf_dict_z, key=tfidf_dict_z.get, reverse=True)[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./tfidf_data/top_words_r.pkl', 'wb') as file:\n",
    "    pickle.dump(top_words_r, file)\n",
    "with open('./tfidf_data/top_words_z.pkl', 'wb') as file:\n",
    "    pickle.dump(top_words_z, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tf-idf 결과 상위 200개 단어에 해당하는 단어들만 남김\n",
    "r1_words_v6=[[word for word in r1_words_v5[i] if word in top_words_r] for i in range(len(r1_words_v5))]\n",
    "r2_words_v6=[[word for word in r2_words_v5[i] if word in top_words_r] for i in range(len(r2_words_v5))]\n",
    "r3_words_v6=[[word for word in r3_words_v5[i] if word in top_words_r] for i in range(len(r3_words_v5))]\n",
    "r4_words_v6=[[word for word in r4_words_v5[i] if word in top_words_r] for i in range(len(r4_words_v5))]\n",
    "r5_words_v6=[[word for word in r5_words_v5[i] if word in top_words_r] for i in range(len(r5_words_v5))]\n",
    "z_words_v6=[[word for word in z_words_v5[i] if word in top_words_z] for i in range(len(z_words_v5))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 엣지 구성 못하는 리스트 제거, 나머지 정렬\n",
    "r1_words_v7=[sorted(words) for words in r1_words_v6 if len(words)>1]\n",
    "r2_words_v7=[sorted(words) for words in r2_words_v6 if len(words)>1]\n",
    "r3_words_v7=[sorted(words) for words in r3_words_v6 if len(words)>1]\n",
    "r4_words_v7=[sorted(words) for words in r4_words_v6 if len(words)>1]\n",
    "r5_words_v7=[sorted(words) for words in r5_words_v6 if len(words)>1]\n",
    "z_words_v7=[sorted(words) for words in z_words_v6 if len(words)>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./tfidf_data/r1_words_v7.pkl', 'wb') as file:\n",
    "    pickle.dump(r1_words_v7, file)\n",
    "with open('./tfidf_data/r2_words_v7.pkl', 'wb') as file:\n",
    "    pickle.dump(r2_words_v7, file)\n",
    "with open('./tfidf_data/r3_words_v7.pkl', 'wb') as file:\n",
    "    pickle.dump(r3_words_v7, file)\n",
    "with open('./tfidf_data/r4_words_v7.pkl', 'wb') as file:\n",
    "    pickle.dump(r4_words_v7, file)\n",
    "with open('./tfidf_data/r5_words_v7.pkl', 'wb') as file:\n",
    "    pickle.dump(r5_words_v7, file)\n",
    "with open('./tfidf_data/z_words_v7.pkl', 'wb') as file:\n",
    "    pickle.dump(z_words_v7, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('./tfidf_data/r1_words_v7.pkl', 'rb') as file:\n",
    "#     r1_words_v7 = pickle.load(file)\n",
    "# with open('./tfidf_data/r2_words_v7.pkl', 'rb') as file:\n",
    "#     r2_words_v7 = pickle.load(file)\n",
    "# with open('./tfidf_data/r3_words_v7.pkl', 'rb') as file:\n",
    "#     r3_words_v7 = pickle.load(file)\n",
    "# with open('./tfidf_data/r4_words_v7.pkl', 'rb') as file:\n",
    "#     r4_words_v7 = pickle.load(file)\n",
    "# with open('./tfidf_data/r5_words_v7.pkl', 'rb') as file:\n",
    "#     r5_words_v7 = pickle.load(file)\n",
    "# with open('./tfidf_data/z_words_v7.pkl', 'rb') as file:\n",
    "#     z_words_v7 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5452/5452 [00:00<00:00, 287487.68it/s]\n",
      "100%|██████████| 3143/3143 [00:00<00:00, 524580.08it/s]\n",
      "100%|██████████| 1646/1646 [00:00<00:00, 413799.11it/s]\n",
      "100%|██████████| 745/745 [00:00<00:00, 695936.86it/s]\n",
      "100%|██████████| 235/235 [00:00<00:00, 648035.13it/s]\n",
      "100%|██████████| 530/530 [00:00<00:00, 503848.85it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "rob_edges=[]\n",
    "zep_edges=[]\n",
    "\n",
    "def edge_extracter(df,edges):\n",
    "    for word_list in tqdm(df):\n",
    "        for i in range(len(word_list)):\n",
    "            for j in range(len(word_list)):\n",
    "                if i<j:\n",
    "                    edges.append([word_list[i],word_list[j]])\n",
    "    return edges\n",
    "\n",
    "rob_edges=edge_extracter(r1_words_v7,rob_edges)\n",
    "rob_edges=edge_extracter(r2_words_v7,rob_edges)\n",
    "rob_edges=edge_extracter(r3_words_v7,rob_edges)\n",
    "rob_edges=edge_extracter(r4_words_v7,rob_edges)\n",
    "rob_edges=edge_extracter(r5_words_v7,rob_edges)\n",
    "zep_edges=edge_extracter(z_words_v7,zep_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_mat_r=np.zeros((200,200))\n",
    "adjacency_mat_z=np.zeros((200,200))\n",
    "\n",
    "\n",
    "top_words_r=np.array(top_words_r)\n",
    "top_words_z=np.array(top_words_z)\n",
    "\n",
    "for [a,b] in rob_edges:\n",
    "    a_ind=np.where(top_words_r==a)[0]\n",
    "    b_ind=np.where(top_words_r==b)[0]\n",
    "    adjacency_mat_r[a_ind,b_ind]+=1\n",
    "\n",
    "for [a,b] in zep_edges:\n",
    "    a_ind=np.where(top_words_z==a)[0]\n",
    "    b_ind=np.where(top_words_z==b)[0]\n",
    "    adjacency_mat_z[a_ind,b_ind]+=1\n",
    "\n",
    "# 그래프 edge 구성\n",
    "\n",
    "edge_r=[]\n",
    "\n",
    "for i,node_a in enumerate(top_words_r):\n",
    "    for j,node_b in enumerate(top_words_r):\n",
    "        if i<j:\n",
    "            edge_r.append([node_a,node_b,{'weight':adjacency_mat_r[i,j]}])\n",
    "\n",
    "edge_z=[]\n",
    "\n",
    "for i,node_a in enumerate(top_words_z):\n",
    "    for j,node_b in enumerate(top_words_z):\n",
    "        if i<j:\n",
    "            edge_z.append([node_a,node_b,{'weight':adjacency_mat_z[i,j]}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_r_v2=[each for each in edge_r if each[2]['weight']>0]\n",
    "edge_z_v2=[each for each in edge_z if each[2]['weight']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198, 85)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(edge_r_v2),len(edge_z_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 노드 구성\n",
    "\n",
    "node_r=[]\n",
    "for node in top_words_r:\n",
    "    node_r.append([node,{'label':node}])\n",
    "\n",
    "node_z=[]\n",
    "for node in top_words_z:\n",
    "    node_z.append([node,{'label':node}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# 그래프 구성\n",
    "\n",
    "G_r = nx.Graph()\n",
    "G_r.add_nodes_from(node_r)\n",
    "G_r.add_edges_from(edge_r_v2)\n",
    "\n",
    "G_z = nx.Graph()\n",
    "G_z.add_nodes_from(node_z)\n",
    "G_z.add_edges_from(edge_z_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GraphML 파일로 저장\n",
    "nx.write_graphml(G_r, \"./tfidf_data/G_r.graphml\")\n",
    "nx.write_graphml(G_z, \"./tfidf_data/G_z.graphml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 여기까지 그래프 생성 코드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "es_py38",
   "language": "python",
   "name": "es_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
