{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-05 15:11:16.187988: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-05 15:11:16.239827: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "from random import randint\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopword_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morphs(text, noun = True, verb = False, adjective = False, adverb = False):\n",
    "    poses = pos_tag(text, tagset = 'universal')\n",
    "    filters = []\n",
    "\n",
    "    if noun:\n",
    "        filters.append('NOUN')\n",
    "    if verb:\n",
    "        filters.append('VERB')\n",
    "    if adjective:\n",
    "        filters.append('ADJ')\n",
    "    if adverb:\n",
    "        filters.append('ADV')\n",
    "\n",
    "    return [pos[0] for pos in poses if pos[1] in filters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('./datasets/original/roblox1.csv', index_col = 0, low_memory = False)\n",
    "df2 = pd.read_csv('./datasets/original/roblox2.csv', index_col = 0, low_memory = False)\n",
    "df3 = pd.read_csv('./datasets/original/roblox3.csv', index_col = 0, low_memory = False)\n",
    "df4 = pd.read_csv('./datasets/original/roblox4.csv', index_col = 0, low_memory = False)\n",
    "df5 = pd.read_csv('./datasets/original/roblox5.csv', index_col = 0, low_memory = False)\n",
    "df6 = pd.read_csv('./datasets/original/zepeto.csv', index_col = 0, low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2, df3, df4, df5, df6]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:35<00:00, 2822.25it/s]\n",
      "100%|██████████| 100000/100000 [00:00<00:00, 110416.00it/s]\n",
      "100%|██████████| 100000/100000 [00:21<00:00, 4752.99it/s]\n",
      "100%|██████████| 100000/100000 [00:01<00:00, 77533.73it/s]\n",
      "100%|██████████| 100000/100000 [00:00<00:00, 113578.99it/s]\n",
      "100%|██████████| 100000/100000 [02:25<00:00, 688.46it/s]\n",
      "100%|██████████| 100000/100000 [00:04<00:00, 21862.54it/s]\n",
      "100%|██████████| 100000/100000 [00:00<00:00, 604001.04it/s]\n",
      "100%|██████████| 100000/100000 [00:26<00:00, 3841.03it/s]\n",
      "100%|██████████| 100000/100000 [00:00<00:00, 129448.27it/s]\n",
      "100%|██████████| 100000/100000 [00:15<00:00, 6371.53it/s]\n",
      "100%|██████████| 100000/100000 [00:00<00:00, 161741.47it/s]\n",
      "100%|██████████| 100000/100000 [00:00<00:00, 128093.08it/s]\n",
      "100%|██████████| 100000/100000 [02:00<00:00, 828.49it/s]\n",
      "100%|██████████| 100000/100000 [00:03<00:00, 30819.96it/s]\n",
      "100%|██████████| 100000/100000 [00:00<00:00, 632388.49it/s]\n",
      "100%|██████████| 100000/100000 [00:19<00:00, 5110.54it/s]\n",
      "100%|██████████| 100000/100000 [00:01<00:00, 79395.83it/s]\n",
      "100%|██████████| 100000/100000 [00:11<00:00, 8745.64it/s]\n",
      "100%|██████████| 100000/100000 [00:00<00:00, 192884.66it/s]\n",
      "100%|██████████| 100000/100000 [00:01<00:00, 85097.89it/s]\n",
      "100%|██████████| 100000/100000 [01:35<00:00, 1052.49it/s]\n",
      "100%|██████████| 100000/100000 [00:02<00:00, 38309.38it/s]\n",
      "100%|██████████| 100000/100000 [00:00<00:00, 608792.16it/s]\n",
      "100%|██████████| 100000/100000 [00:15<00:00, 6387.91it/s]\n",
      "100%|██████████| 100000/100000 [00:00<00:00, 200898.66it/s]\n",
      "100%|██████████| 100000/100000 [00:08<00:00, 11415.83it/s]\n",
      "100%|██████████| 100000/100000 [00:00<00:00, 242281.88it/s]\n",
      "100%|██████████| 100000/100000 [00:00<00:00, 198828.92it/s]\n",
      "100%|██████████| 100000/100000 [01:09<00:00, 1432.19it/s]\n",
      "100%|██████████| 100000/100000 [00:01<00:00, 50897.07it/s]\n",
      "100%|██████████| 100000/100000 [00:00<00:00, 677139.56it/s]\n",
      "100%|██████████| 46700/46700 [00:06<00:00, 7050.82it/s]\n",
      "100%|██████████| 46700/46700 [00:00<00:00, 228925.59it/s]\n",
      "100%|██████████| 46700/46700 [00:03<00:00, 11774.51it/s]\n",
      "100%|██████████| 46700/46700 [00:00<00:00, 261842.63it/s]\n",
      "100%|██████████| 46700/46700 [00:00<00:00, 210538.65it/s]\n",
      "100%|██████████| 46700/46700 [00:31<00:00, 1498.17it/s]\n",
      "100%|██████████| 46700/46700 [00:00<00:00, 54861.47it/s]\n",
      "100%|██████████| 46700/46700 [00:00<00:00, 586305.15it/s]\n",
      "100%|██████████| 115407/115407 [00:16<00:00, 6972.28it/s] \n",
      "100%|██████████| 115407/115407 [00:00<00:00, 267148.09it/s]\n",
      "100%|██████████| 115407/115407 [00:06<00:00, 18168.37it/s]\n",
      "100%|██████████| 115407/115407 [00:00<00:00, 129460.54it/s]\n",
      "100%|██████████| 115407/115407 [00:00<00:00, 254960.15it/s]\n",
      "100%|██████████| 115407/115407 [01:02<00:00, 1860.83it/s]\n",
      "100%|██████████| 115407/115407 [00:01<00:00, 68820.09it/s] \n",
      "100%|██████████| 115407/115407 [00:00<00:00, 735271.53it/s]\n"
     ]
    }
   ],
   "source": [
    "for df in [df1, df2, df3, df4, df5, df6]:\n",
    "    df['keywords'] = df['content'].progress_apply(word_tokenize)                                                             # Tokenize\n",
    "    df['keywords'] = df['keywords'].progress_apply(lambda x : [y for y in x if y.isalpha()])                                  # exclude if numeric\n",
    "    df['keywords'] = df['keywords'].progress_apply(lambda x : [lemmatizer.lemmatize(y) for y in x])                           # lemmatize\n",
    "    df['keywords'] = df['keywords'].progress_apply(lambda x : list(set(x)))                                                   # drop duplicate\n",
    "    df['keywords'] = df['keywords'].progress_apply(lambda x : [y for y in x if len(y) >= 3 and len(y) <= 15])                 # 3 <= len(keyword) <= 15\n",
    "    df['keywords'] = df['keywords'].progress_apply(morphs, noun = True, verb = True, adjective = False, adverb = False)       # select noun\n",
    "    df['keywords'] = df['keywords'].progress_apply(lambda x : [y for y in x if y not in stopword_list])                       # stopwords\n",
    "    df['keywords'] = df['keywords'].progress_apply(lambda x : ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('./datasets/tfidf/roblox1.csv')\n",
    "df2.to_csv('./datasets/tfidf/roblox2.csv')\n",
    "df3.to_csv('./datasets/tfidf/roblox3.csv')\n",
    "df4.to_csv('./datasets/tfidf/roblox4.csv')\n",
    "df5.to_csv('./datasets/tfidf/roblox5.csv')\n",
    "df6.to_csv('./datasets/tfidf/zepeto.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
