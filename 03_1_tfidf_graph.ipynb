{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENVIRON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm, trange\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stopword_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(container):\n",
    "    for i in container:\n",
    "        if isinstance(i, (list,tuple)):\n",
    "            for j in flatten(i):\n",
    "                yield j\n",
    "        else:\n",
    "            yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morphs(text, noun = True, verb = False, adjective = False, adverb = False):\n",
    "    poses = pos_tag(text, tagset = 'universal')\n",
    "    filters = []\n",
    "\n",
    "    if noun:\n",
    "        filters.append('NOUN')\n",
    "    if verb:\n",
    "        filters.append('VERB')\n",
    "    if adjective:\n",
    "        filters.append('ADJ')\n",
    "    if adverb:\n",
    "        filters.append('ADV')\n",
    "\n",
    "    return [pos[0] for pos in poses if pos[1] in filters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COUNT OVER 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445931/445931 [00:01<00:00, 303994.14it/s]\n",
      "100%|██████████| 445931/445931 [00:01<00:00, 264014.70it/s]\n",
      "100%|██████████| 445931/445931 [00:16<00:00, 27134.55it/s]\n",
      "100%|██████████| 445931/445931 [00:01<00:00, 342368.39it/s]\n",
      "100%|██████████| 445931/445931 [00:01<00:00, 282027.95it/s]\n",
      "100%|██████████| 445931/445931 [03:47<00:00, 1960.05it/s]\n",
      "100%|██████████| 445931/445931 [00:00<00:00, 956335.40it/s]\n",
      "100%|██████████| 445931/445931 [02:16<00:00, 3259.43it/s]\n",
      "100%|██████████| 8300/8300 [00:00<00:00, 214816.44it/s]\n",
      "100%|██████████| 445931/445931 [01:25<00:00, 5194.06it/s] \n",
      "100%|██████████| 8300/8300 [00:00<00:00, 224266.88it/s]\n",
      "100%|██████████| 19817863/19817863 [00:40<00:00, 485862.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# DATASET - ROBLOX\n",
    "roblox1_df = pd.read_csv(f'./datasets/tfidf/roblox1.csv', index_col = 0, low_memory = False)\n",
    "roblox2_df = pd.read_csv(f'./datasets/tfidf/roblox2.csv', index_col = 0, low_memory = False)\n",
    "roblox3_df = pd.read_csv(f'./datasets/tfidf/roblox3.csv', index_col = 0, low_memory = False)\n",
    "roblox4_df = pd.read_csv(f'./datasets/tfidf/roblox4.csv', index_col = 0, low_memory = False)\n",
    "roblox5_df = pd.read_csv(f'./datasets/tfidf/roblox5.csv', index_col = 0, low_memory = False)\n",
    "\n",
    "df = pd.concat([roblox1_df, roblox2_df, roblox3_df, roblox4_df, roblox5_df]).dropna(subset = ['keywords']).reset_index(drop = True)\n",
    "\n",
    "# PREPROCESS\n",
    "df['keywords'] = df['keywords'].progress_apply(lambda x : x.split())\n",
    "df['keywords'] = df['keywords'].progress_apply(lambda x : [y for y in x if y.isalpha()])                                # exclude if numeric\n",
    "df['keywords'] = df['keywords'].progress_apply(lambda x : [lemmatizer.lemmatize(y) for y in x])                         # lemmatize\n",
    "df['keywords'] = df['keywords'].progress_apply(lambda x : list(set(x)))                                                 # drop duplicate\n",
    "df['keywords'] = df['keywords'].progress_apply(lambda x : [y for y in x if len(y) >= 3 and len(y) <= 15])               # 3 <= len(keyword) <= 15\n",
    "df['keywords'] = df['keywords'].progress_apply(morphs, noun = True, verb = True, adjective = False, adverb = False)     # select noun\n",
    "\n",
    "# OPTIONAL: TF-IDF FILTERING\n",
    "documents = df['keywords'].progress_apply(lambda x : ' '.join(x))\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "words = vectorizer.get_feature_names_out()\n",
    "filter_num = 200\n",
    "\n",
    "tfidf_dict = {}\n",
    "for doc_idx, doc in enumerate(tqdm(documents)):\n",
    "    feature_idx = tfidf_matrix[doc_idx].nonzero()[1]\n",
    "    tfidf_scores = zip(feature_idx, [tfidf_matrix[doc_idx, x] for x in feature_idx])\n",
    "    for word_idx, score in tfidf_scores:\n",
    "        word = words[word_idx]\n",
    "        tfidf_dict[word] = score\n",
    "\n",
    "tfidf_dict = {key: value for value, key in sorted([(score, word) for word, score in tfidf_dict.items()], reverse = True)}\n",
    "\n",
    "# GRAPH FORMULATION - ROBLOX\n",
    "G = nx.MultiGraph()\n",
    "counts = df['keywords'].explode().reset_index(drop = True).reset_index().groupby('keywords').count()['index']\n",
    "counts = counts[counts > 10]\n",
    "\n",
    "# ADD NODES\n",
    "for item in tqdm(counts.index):\n",
    "    G.add_node(item, weight = counts[item])\n",
    "\n",
    "# ADD EDGES\n",
    "for keywords in tqdm(df['keywords']):\n",
    "    keyword_selected = [keyword for keyword in keywords if keyword in counts.index]\n",
    "\n",
    "    pairs = list(combinations(keyword_selected, 2))\n",
    "    pairs = [(pair[0], pair[1]) if pair[0] < pair[1] else (pair[1], pair[0]) for pair in pairs]\n",
    "    \n",
    "    G.add_edges_from(pairs)\n",
    "\n",
    "# MULTI to SINGLE - ROBLOX\n",
    "H = nx.Graph()\n",
    "\n",
    "# MULTI to SINGLE - ADD NODES\n",
    "for item in tqdm(counts.index):\n",
    "    H.add_node(item, weight = counts[item])\n",
    "\n",
    "# MULTI to SINGLE - ADD EDGES\n",
    "for u, v, data in tqdm(G.edges(data = True)):\n",
    "    w = data['weight'] if 'weight' in data else 1.0\n",
    "    if H.has_edge(u, v):\n",
    "        H[u][v]['weight'] += w\n",
    "    else:\n",
    "        H.add_edge(u, v, weight=w)\n",
    "\n",
    "# WRITE GRAPH\n",
    "nx.write_graphml_lxml(H, f'./graph/roblox-tfidf10.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99015/99015 [00:00<00:00, 674031.33it/s]\n",
      "100%|██████████| 99015/99015 [00:00<00:00, 537239.48it/s]\n",
      "100%|██████████| 99015/99015 [00:01<00:00, 54296.00it/s]\n",
      "100%|██████████| 99015/99015 [00:00<00:00, 591154.46it/s]\n",
      "100%|██████████| 99015/99015 [00:00<00:00, 447369.11it/s]\n",
      "100%|██████████| 99015/99015 [00:32<00:00, 3000.46it/s]\n",
      "100%|██████████| 99015/99015 [00:00<00:00, 860190.29it/s]\n",
      "100%|██████████| 99015/99015 [00:23<00:00, 4252.45it/s]\n",
      "100%|██████████| 3072/3072 [00:00<00:00, 222387.37it/s]\n",
      "100%|██████████| 99015/99015 [00:17<00:00, 5610.24it/s] \n",
      "100%|██████████| 3072/3072 [00:00<00:00, 212839.90it/s]\n",
      "100%|██████████| 1786651/1786651 [00:03<00:00, 459031.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# DATASET - ZEPETO\n",
    "df = pd.read_csv(f'./datasets/tfidf/zepeto.csv', index_col = 0, low_memory = False)\n",
    "\n",
    "# PREPROCESS\n",
    "df = df.dropna(subset = ['keywords'])\n",
    "df['keywords'] = df['keywords'].progress_apply(lambda x : x.split())\n",
    "df['keywords'] = df['keywords'].progress_apply(lambda x : [y for y in x if y.isalpha()])                                # exclude if numeric\n",
    "df['keywords'] = df['keywords'].progress_apply(lambda x : [lemmatizer.lemmatize(y) for y in x])                         # lemmatize\n",
    "df['keywords'] = df['keywords'].progress_apply(lambda x : list(set(x)))                                                 # drop duplicate\n",
    "df['keywords'] = df['keywords'].progress_apply(lambda x : [y for y in x if len(y) >= 3 and len(y) <= 15])               # 3 <= len(keyword) <= 15\n",
    "df['keywords'] = df['keywords'].progress_apply(morphs, noun = True, verb = True, adjective = False, adverb = False)     # select noun\n",
    "\n",
    "# OPTIONAL: TF-IDF FILTERING\n",
    "documents = df['keywords'].progress_apply(lambda x : ' '.join(x))\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "words = vectorizer.get_feature_names_out()\n",
    "filter_num = 200\n",
    "\n",
    "tfidf_dict = {}\n",
    "for doc_idx, doc in enumerate(tqdm(documents)):\n",
    "    feature_idx = tfidf_matrix[doc_idx].nonzero()[1]\n",
    "    tfidf_scores = zip(feature_idx, [tfidf_matrix[doc_idx, x] for x in feature_idx])\n",
    "    for word_idx, score in tfidf_scores:\n",
    "        word = words[word_idx]\n",
    "        tfidf_dict[word] = score\n",
    "\n",
    "tfidf_dict = {key: value for value, key in sorted([(score, word) for word, score in tfidf_dict.items()], reverse = True)}\n",
    "\n",
    "# GRAPH FORMULATION - ZEPETO\n",
    "G = nx.MultiGraph()\n",
    "counts = df['keywords'].explode().reset_index(drop = True).reset_index().groupby('keywords').count()['index']\n",
    "counts = counts[counts > 10]\n",
    "\n",
    "# ADD NODES\n",
    "for item in tqdm(counts.index):\n",
    "    G.add_node(item, weight = counts[item])\n",
    "    \n",
    "# ADD EDGES\n",
    "for keywords in tqdm(df['keywords']):\n",
    "    keyword_selected = [keyword for keyword in keywords if keyword in counts.index]\n",
    "\n",
    "    pairs = list(combinations(keyword_selected, 2))\n",
    "    pairs = [(pair[0], pair[1]) if pair[0] < pair[1] else (pair[1], pair[0]) for pair in pairs]\n",
    "    \n",
    "    G.add_edges_from(pairs)\n",
    "\n",
    "# MULTI to SINGLE - ZEPETO\n",
    "H = nx.Graph()\n",
    "\n",
    "# MULTI to SINGLE - ADD NODES\n",
    "for item in tqdm(counts.index):\n",
    "    H.add_node(item, weight = counts[item])\n",
    "\n",
    "# MULTI to SINGLE - ADD EDGES\n",
    "for u, v, data in tqdm(G.edges(data = True)):\n",
    "    w = data['weight'] if 'weight' in data else 1.0\n",
    "    if H.has_edge(u, v):\n",
    "        H[u][v]['weight'] += w\n",
    "    else:\n",
    "        H.add_edge(u, v, weight=w)\n",
    "\n",
    "# WRITE GRAPH\n",
    "nx.write_graphml_lxml(H, f'./graph/zepeto-tfidf10.graphml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WITHOUT FILTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445931/445931 [00:03<00:00, 129352.84it/s]\n",
      "100%|██████████| 445931/445931 [00:01<00:00, 223551.33it/s]\n",
      "100%|██████████| 445931/445931 [00:15<00:00, 29180.72it/s]\n",
      "100%|██████████| 445931/445931 [00:01<00:00, 223730.04it/s]\n",
      "100%|██████████| 445931/445931 [00:02<00:00, 207243.90it/s]\n",
      "100%|██████████| 445931/445931 [03:50<00:00, 1938.77it/s]\n",
      "100%|██████████| 445931/445931 [00:00<00:00, 887612.60it/s]\n",
      "100%|██████████| 445931/445931 [02:17<00:00, 3240.44it/s]\n",
      "100%|██████████| 76990/76990 [00:00<00:00, 209502.81it/s]\n",
      "100%|██████████| 445931/445931 [01:38<00:00, 4514.13it/s] \n",
      "100%|██████████| 76990/76990 [00:00<00:00, 206687.37it/s]\n",
      "100%|██████████| 21413808/21413808 [00:48<00:00, 443371.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# DATASET - ROBLOX\n",
    "roblox1_df = pd.read_csv(f'./datasets/tfidf/roblox1.csv', index_col = 0, low_memory = False)\n",
    "roblox2_df = pd.read_csv(f'./datasets/tfidf/roblox2.csv', index_col = 0, low_memory = False)\n",
    "roblox3_df = pd.read_csv(f'./datasets/tfidf/roblox3.csv', index_col = 0, low_memory = False)\n",
    "roblox4_df = pd.read_csv(f'./datasets/tfidf/roblox4.csv', index_col = 0, low_memory = False)\n",
    "roblox5_df = pd.read_csv(f'./datasets/tfidf/roblox5.csv', index_col = 0, low_memory = False)\n",
    "\n",
    "df = pd.concat([roblox1_df, roblox2_df, roblox3_df, roblox4_df, roblox5_df]).dropna(subset = ['keywords']).reset_index(drop = True)\n",
    "\n",
    "# PREPROCESS\n",
    "df['keywords'] = df['keywords'].progress_apply(lambda x : x.split())\n",
    "df['keywords'] = df['keywords'].progress_apply(lambda x : [y for y in x if y.isalpha()])                                # exclude if numeric\n",
    "df['keywords'] = df['keywords'].progress_apply(lambda x : [lemmatizer.lemmatize(y) for y in x])                         # lemmatize\n",
    "df['keywords'] = df['keywords'].progress_apply(lambda x : list(set(x)))                                                 # drop duplicate\n",
    "df['keywords'] = df['keywords'].progress_apply(lambda x : [y for y in x if len(y) >= 3 and len(y) <= 15])               # 3 <= len(keyword) <= 15\n",
    "df['keywords'] = df['keywords'].progress_apply(morphs, noun = True, verb = True, adjective = False, adverb = False)     # select noun\n",
    "\n",
    "# OPTIONAL: TF-IDF FILTERING\n",
    "documents = df['keywords'].progress_apply(lambda x : ' '.join(x))\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "words = vectorizer.get_feature_names_out()\n",
    "filter_num = 200\n",
    "\n",
    "tfidf_dict = {}\n",
    "for doc_idx, doc in enumerate(tqdm(documents)):\n",
    "    feature_idx = tfidf_matrix[doc_idx].nonzero()[1]\n",
    "    tfidf_scores = zip(feature_idx, [tfidf_matrix[doc_idx, x] for x in feature_idx])\n",
    "    for word_idx, score in tfidf_scores:\n",
    "        word = words[word_idx]\n",
    "        tfidf_dict[word] = score\n",
    "\n",
    "tfidf_dict = {key: value for value, key in sorted([(score, word) for word, score in tfidf_dict.items()], reverse = True)}\n",
    "\n",
    "# GRAPH FORMULATION - ROBLOX\n",
    "G = nx.MultiGraph()\n",
    "counts = df['keywords'].explode().reset_index(drop = True).reset_index().groupby('keywords').count()['index']\n",
    "# counts = counts[counts > 10]\n",
    "\n",
    "# ADD NODES\n",
    "for item in tqdm(counts.index):\n",
    "    G.add_node(item, weight = counts[item])\n",
    "\n",
    "# ADD EDGES\n",
    "for keywords in tqdm(df['keywords']):\n",
    "    keyword_selected = [keyword for keyword in keywords if keyword in counts.index]\n",
    "\n",
    "    pairs = list(combinations(keyword_selected, 2))\n",
    "    pairs = [(pair[0], pair[1]) if pair[0] < pair[1] else (pair[1], pair[0]) for pair in pairs]\n",
    "    \n",
    "    G.add_edges_from(pairs)\n",
    "\n",
    "# MULTI to SINGLE - ROBLOX\n",
    "H = nx.Graph()\n",
    "\n",
    "# MULTI to SINGLE - ADD NODES\n",
    "for item in tqdm(counts.index):\n",
    "    H.add_node(item, weight = counts[item])\n",
    "\n",
    "# MULTI to SINGLE - ADD EDGES\n",
    "for u, v, data in tqdm(G.edges(data = True)):\n",
    "    w = data['weight'] if 'weight' in data else 1.0\n",
    "    if H.has_edge(u, v):\n",
    "        H[u][v]['weight'] += w\n",
    "    else:\n",
    "        H.add_edge(u, v, weight=w)\n",
    "\n",
    "# WRITE GRAPH\n",
    "nx.write_graphml_lxml(H, f'./graph/roblox-tfidf.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99015/99015 [00:00<00:00, 682901.46it/s]\n",
      "100%|██████████| 99015/99015 [00:00<00:00, 538779.10it/s]\n",
      "100%|██████████| 99015/99015 [00:01<00:00, 53687.86it/s]\n",
      "100%|██████████| 99015/99015 [00:00<00:00, 567856.50it/s]\n",
      "100%|██████████| 99015/99015 [00:00<00:00, 412753.97it/s]\n",
      "100%|██████████| 99015/99015 [00:33<00:00, 2983.06it/s]\n",
      "100%|██████████| 99015/99015 [00:00<00:00, 871210.36it/s]\n",
      "100%|██████████| 99015/99015 [00:23<00:00, 4246.05it/s]\n",
      "100%|██████████| 39121/39121 [00:00<00:00, 211388.65it/s]\n",
      "100%|██████████| 99015/99015 [00:24<00:00, 3985.18it/s] \n",
      "100%|██████████| 39121/39121 [00:00<00:00, 213979.36it/s]\n",
      "100%|██████████| 2351277/2351277 [00:05<00:00, 395663.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# DATASET - ZEPETO\n",
    "df = pd.read_csv(f'./datasets/tfidf/zepeto.csv', index_col = 0, low_memory = False)\n",
    "\n",
    "# PREPROCESS\n",
    "df = df.dropna(subset = ['keywords'])\n",
    "df['keywords'] = df['keywords'].progress_apply(lambda x : x.split())\n",
    "df['keywords'] = df['keywords'].progress_apply(lambda x : [y for y in x if y.isalpha()])                                # exclude if numeric\n",
    "df['keywords'] = df['keywords'].progress_apply(lambda x : [lemmatizer.lemmatize(y) for y in x])                         # lemmatize\n",
    "df['keywords'] = df['keywords'].progress_apply(lambda x : list(set(x)))                                                 # drop duplicate\n",
    "df['keywords'] = df['keywords'].progress_apply(lambda x : [y for y in x if len(y) >= 3 and len(y) <= 15])               # 3 <= len(keyword) <= 15\n",
    "df['keywords'] = df['keywords'].progress_apply(morphs, noun = True, verb = True, adjective = False, adverb = False)     # select noun\n",
    "\n",
    "# OPTIONAL: TF-IDF FILTERING\n",
    "documents = df['keywords'].progress_apply(lambda x : ' '.join(x))\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "words = vectorizer.get_feature_names_out()\n",
    "filter_num = 200\n",
    "\n",
    "tfidf_dict = {}\n",
    "for doc_idx, doc in enumerate(tqdm(documents)):\n",
    "    feature_idx = tfidf_matrix[doc_idx].nonzero()[1]\n",
    "    tfidf_scores = zip(feature_idx, [tfidf_matrix[doc_idx, x] for x in feature_idx])\n",
    "    for word_idx, score in tfidf_scores:\n",
    "        word = words[word_idx]\n",
    "        tfidf_dict[word] = score\n",
    "\n",
    "tfidf_dict = {key: value for value, key in sorted([(score, word) for word, score in tfidf_dict.items()], reverse = True)}\n",
    "\n",
    "# GRAPH FORMULATION - ZEPETO\n",
    "G = nx.MultiGraph()\n",
    "counts = df['keywords'].explode().reset_index(drop = True).reset_index().groupby('keywords').count()['index']\n",
    "# counts = counts[counts > 10]\n",
    "\n",
    "# ADD NODES\n",
    "for item in tqdm(counts.index):\n",
    "    G.add_node(item, weight = counts[item])\n",
    "    \n",
    "# ADD EDGES\n",
    "for keywords in tqdm(df['keywords']):\n",
    "    keyword_selected = [keyword for keyword in keywords if keyword in counts.index]\n",
    "\n",
    "    pairs = list(combinations(keyword_selected, 2))\n",
    "    pairs = [(pair[0], pair[1]) if pair[0] < pair[1] else (pair[1], pair[0]) for pair in pairs]\n",
    "    \n",
    "    G.add_edges_from(pairs)\n",
    "\n",
    "# MULTI to SINGLE - ZEPETO\n",
    "H = nx.Graph()\n",
    "\n",
    "# MULTI to SINGLE - ADD NODES\n",
    "for item in tqdm(counts.index):\n",
    "    H.add_node(item, weight = counts[item])\n",
    "\n",
    "# MULTI to SINGLE - ADD EDGES\n",
    "for u, v, data in tqdm(G.edges(data = True)):\n",
    "    w = data['weight'] if 'weight' in data else 1.0\n",
    "    if H.has_edge(u, v):\n",
    "        H[u][v]['weight'] += w\n",
    "    else:\n",
    "        H.add_edge(u, v, weight=w)\n",
    "\n",
    "# WRITE GRAPH\n",
    "nx.write_graphml_lxml(H, f'./graph/zepeto-tfidf.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
