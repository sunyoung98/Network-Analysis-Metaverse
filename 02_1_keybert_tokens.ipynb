{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "from random import randint\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, filter = ['NN', 'NNS', 'NNP', 'NNPS']):\n",
    "    # NOUN: 'NN', 'NNS', 'NNP', 'NNPS',\n",
    "    # VERB: 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ',\n",
    "    # ADJS: 'JJ', 'JJR', 'JJS',\n",
    "    # ADVS: 'RB', 'RBR', 'RBS',\n",
    "\n",
    "    text = [lemmatizer.lemmatize(word.lower()) for word in word_tokenize(text)]                     # Tokenization & Lemmatization\n",
    "    text = [word for word in text if word not in stopwords.words('english') and word.isalpha()]     # Exclude Stopwords & Non-English words\n",
    "    text = [word for word, tag in pos_tag(text) if tag in filter]                                   # Filter words by tags.\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    'all-mpnet-base-v2',\n",
    "    'multi-qa-mpnet-base-dot-v1',\n",
    "    'all-distilroberta-v1',\n",
    "    'all-MiniLM-L12-v2',\n",
    "    'multi-qa-distilbert-cos-v1'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models:\n",
    "    print(f'TRAINING ON {model_name}')\n",
    "\n",
    "    model = KeyBERT(model_name)\n",
    "    \n",
    "    df1 = pd.read_csv('./datasets/original/roblox1.csv', index_col = 0, low_memory = False)\n",
    "    df2 = pd.read_csv('./datasets/original/roblox2.csv', index_col = 0, low_memory = False)\n",
    "    df3 = pd.read_csv('./datasets/original/roblox3.csv', index_col = 0, low_memory = False)\n",
    "    df4 = pd.read_csv('./datasets/original/roblox4.csv', index_col = 0, low_memory = False)\n",
    "    df5 = pd.read_csv('./datasets/original/roblox5.csv', index_col = 0, low_memory = False)\n",
    "    df6 = pd.read_csv('./datasets/original/zepeto.csv', index_col = 0, low_memory = False)\n",
    "\n",
    "    df1['keybert_keywords'] = df1['content'].progress_apply(lambda x : model.extract_keywords(preprocess(x), top_n = 10))\n",
    "    df2['keybert_keywords'] = df2['content'].progress_apply(lambda x : model.extract_keywords(preprocess(x), top_n = 10))\n",
    "    df3['keybert_keywords'] = df3['content'].progress_apply(lambda x : model.extract_keywords(preprocess(x), top_n = 10))\n",
    "    df4['keybert_keywords'] = df4['content'].progress_apply(lambda x : model.extract_keywords(preprocess(x), top_n = 10))\n",
    "    df5['keybert_keywords'] = df5['content'].progress_apply(lambda x : model.extract_keywords(preprocess(x), top_n = 10))\n",
    "    df6['keybert_keywords'] = df6['content'].progress_apply(lambda x : model.extract_keywords(preprocess(x), top_n = 10))\n",
    "\n",
    "    os.makedirs(f'./datasets/keybert-{model_name}-tokenized')\n",
    "\n",
    "    df1.reset_index(drop = True).to_csv(f'./datasets/keybert-{model_name}-tokenized/roblox1.csv')\n",
    "    df2.reset_index(drop = True).to_csv(f'./datasets/keybert-{model_name}-tokenized/roblox2.csv')\n",
    "    df3.reset_index(drop = True).to_csv(f'./datasets/keybert-{model_name}-tokenized/roblox3.csv')\n",
    "    df4.reset_index(drop = True).to_csv(f'./datasets/keybert-{model_name}-tokenized/roblox4.csv')\n",
    "    df5.reset_index(drop = True).to_csv(f'./datasets/keybert-{model_name}-tokenized/roblox5.csv')\n",
    "    df6.reset_index(drop = True).to_csv(f'./datasets/keybert-{model_name}-tokenized/zepeto.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
